{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "257685f3",
   "metadata": {},
   "source": [
    "## **Car Insurance Analysis**\n",
    "\n",
    "Hey,\n",
    "this project is a bonus material from DataCamp course. In this project, I will fundamentally answer a single question: **which variable is the best predictor of whether a costumer will put in a claim on their car insurance?**\n",
    "According to the project's tale, this variable will be the single feature in a machine learning model to try to correctly predict this outcome. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c3f0e974-faf8-458f-bf2a-06a469d0ea5e",
   "metadata": {},
   "source": [
    "*DataCamp background for the project*\n",
    "\n",
    "Insurance companies invest a lot of time and money into optimizing their pricing and accurately estimating the likelihood that customers will make a claim. In many countries insurance it is a legal requirement to have car insurance in order to drive a vehicle on public roads, so the market is very large!\n",
    "\n",
    "Knowing all of this, On the Road car insurance have requested your services in building a model to predict whether a customer will make a claim on their insurance during the policy period. As they have very little expertise and infrastructure for deploying and monitoring machine learning models, they've asked you to identify the single feature that results in the best performing model, as measured by accuracy, so they can start with a simple model in production.\n",
    "\n",
    "They have supplied you with their customer data as a csv file called `car_insurance.csv`, along with a table detailing the column names and descriptions below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8928ffdf-25d6-4ad9-909f-0dd8d10b9a42",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## The dataset\n",
    "\n",
    "| Column | Description |\n",
    "|--------|-------------|\n",
    "| `id` | Unique client identifier |\n",
    "| `age` | Client's age: <br> <ul><li>`0`: 16-25</li><li>`1`: 26-39</li><li>`2`: 40-64</li><li>`3`: 65+</li></ul> |\n",
    "| `gender` | Client's gender: <br> <ul><li>`0`: Female</li><li>`1`: Male</li></ul> |\n",
    "| `driving_experience` | Years the client has been driving: <br> <ul><li>`0`: 0-9</li><li>`1`: 10-19</li><li>`2`: 20-29</li><li>`3`: 30+</li></ul> |\n",
    "| `education` | Client's level of education: <br> <ul><li>`0`: No education</li><li>`1`: High school</li><li>`2`: University</li></ul> |\n",
    "| `income` | Client's income level: <br> <ul><li>`0`: Poverty</li><li>`1`: Working class</li><li>`2`: Middle class</li><li>`3`: Upper class</li></ul> |\n",
    "| `credit_score` | Client's credit score (between zero and one) |\n",
    "| `vehicle_ownership` | Client's vehicle ownership status: <br><ul><li>`0`: Does not own their vehilce (paying off finance)</li><li>`1`: Owns their vehicle</li></ul> |\n",
    "| `vehcile_year` | Year of vehicle registration: <br><ul><li>`0`: Before 2015</li><li>`1`: 2015 or later</li></ul> |\n",
    "| `married` | Client's marital status: <br><ul><li>`0`: Not married</li><li>`1`: Married</li></ul> |\n",
    "| `children` | Client's number of children |\n",
    "| `postal_code` | Client's postal code | \n",
    "| `annual_mileage` | Number of miles driven by the client each year |\n",
    "| `vehicle_type` | Type of car: <br> <ul><li>`0`: Sedan</li><li>`1`: Sports car</li></ul> |\n",
    "| `speeding_violations` | Total number of speeding violations received by the client | \n",
    "| `duis` | Number of times the client has been caught driving under the influence of alcohol |\n",
    "| `past_accidents` | Total number of previous accidents the client has been involved in |\n",
    "| `outcome` | Whether the client made a claim on their car insurance (response variable): <br><ul><li>`0`: No claim</li><li>`1`: Made a claim</li></ul> |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f49c43",
   "metadata": {},
   "source": [
    "## Importing libs and defining functions\n",
    "\n",
    "To perform this analysis, I will use the pandas and numpy libraries to clean the data and the ```logit``` function from statsmodels to perform the logistic regression. Also, as the exercise didn't grant a tidy data frame, I'll explore the data and check its columns.\n",
    "\n",
    "### Why logit? What's Logit anyway? \n",
    "\n",
    "The dependent variable, also called *response variable*, to be studied is known as binary because it assumes ``0`` or ``1``. When it's ``1``, the client made a claim on their car insurance and ``0`` otherwise. So, what I am measuring here it's the probability change due to the change in one of the independent variables. Logit models are regressions using the logistic function as the underlying probability function. It grants that the probability will be between 0 and 1. That's why I'm importing ``logit`` instead of the ```ols``` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0eb4f16-5a99-460d-a5ba-706b7ef0bbe7",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 11,
    "id": "bA5ajAmk7XH6",
    "lastExecutedAt": 1733432760888,
    "lastExecutedByKernel": "464275e9-0e65-4af8-8f08-a524cdaeee97",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Import required modules\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.formula.api import logit\n"
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.formula.api import logit\n",
    "\n",
    "# Functions to assess the cleaniness of the data\n",
    "def assert_col(df, col, cat):\n",
    "    \"\"\"Assert if a column from a data frame has only the indicate number for categories.\n",
    "    \n",
    "    Args:\n",
    "        df (PandasDataFrame): a PandasDataframe.\n",
    "        col (str): the df's column to be verified.\n",
    "        cat (int): the expected number of categories.\n",
    "        \n",
    "    Returns:\n",
    "        print: a message to indicate if the column is clean, i.e., a codified categorical variable or not.\"\"\"\n",
    "    try:\n",
    "        assert len(df[col].unique()) == cat\n",
    "        print(f'the {col} column match its description.')\n",
    "    except AssertionError as e:\n",
    "        print(f'the {col} column doesn\\'t match its description.')\n",
    "        print(e)\n",
    "\n",
    "def assert_cat(df, col):\n",
    "    \"\"\"Assert if the column is categorical. If not categorical, convert to the categorical type.\n",
    "    \n",
    "    Args:\n",
    "        df (PandasDataFrame): a PandasDataframe.\n",
    "        col (str): the df's column to be verified.\n",
    "        \n",
    "    Returns:\n",
    "        PandasSeries: returns the column in Series format converted to categorical if necessary. \n",
    "        It also displays a message, indicating if the astype method was necessary\"\"\"\n",
    "    col_type = str(df[col].dtype)\n",
    "    if col_type != 'category':\n",
    "        df[col] = df[col].astype('category')\n",
    "        print(f'the {col} column is not category. astype method applied.')\n",
    "        return df[col]\n",
    "    else:\n",
    "        print(f'the {col} column is a category. Returning the column.')\n",
    "        return df[col]\n",
    "\n",
    "# Function to check the logit model accuracy\n",
    "def accuracy(logit_model):\n",
    "    \"\"\"Return the accuracy of the logit model. A logit's model accuracy measure the proportion \n",
    "    of correct predictions of the dataset. It is the ratio between the correct predictions and all\n",
    "    the model predictions.\n",
    "    \n",
    "    Args:\n",
    "        logit_model (logit model): a fitted logit model.\n",
    "        \n",
    "    Returns:\n",
    "        int: the accuracy of the logit model\"\"\"\n",
    "    conf_matrix = logit_model.pred_table()\n",
    "    acc = (conf_matrix[0,0] + conf_matrix[1,1])/(conf_matrix[0,0] + conf_matrix[1,1] + conf_matrix[1,0] + conf_matrix[0,1])\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e1b7b8",
   "metadata": {},
   "source": [
    "## Reading and Exploring the Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10619679",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('car_insurance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "456dc4ce-b016-436b-b065-260dc50fa7c6",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 52,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1733432761699,
    "lastExecutedByKernel": "464275e9-0e65-4af8-8f08-a524cdaeee97",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "df = pd.read_csv('car_insurance.csv')\nprint(df.head())\nprint(df.info())",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id  age  gender driving_experience    education         income  \\\n",
      "0  569520    3       0               0-9y  high school    upper class   \n",
      "1  750365    0       1               0-9y         none        poverty   \n",
      "2  199901    0       0               0-9y  high school  working class   \n",
      "3  478866    0       1               0-9y   university  working class   \n",
      "4  731664    1       1             10-19y         none  working class   \n",
      "\n",
      "   credit_score  vehicle_ownership vehicle_year  married  children  \\\n",
      "0      0.629027                1.0   after 2015      0.0       1.0   \n",
      "1      0.357757                0.0  before 2015      0.0       0.0   \n",
      "2      0.493146                1.0  before 2015      0.0       0.0   \n",
      "3      0.206013                1.0  before 2015      0.0       1.0   \n",
      "4      0.388366                1.0  before 2015      0.0       0.0   \n",
      "\n",
      "   postal_code  annual_mileage vehicle_type  speeding_violations  duis  \\\n",
      "0        10238         12000.0        sedan                    0     0   \n",
      "1        10238         16000.0        sedan                    0     0   \n",
      "2        10238         11000.0        sedan                    0     0   \n",
      "3        32765         11000.0        sedan                    0     0   \n",
      "4        32765         12000.0        sedan                    2     0   \n",
      "\n",
      "   past_accidents  outcome  \n",
      "0               0      0.0  \n",
      "1               0      1.0  \n",
      "2               0      0.0  \n",
      "3               0      0.0  \n",
      "4               1      1.0  \n"
     ]
    }
   ],
   "source": [
    "# df's first five rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaa8bcdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 18 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   id                   10000 non-null  int64  \n",
      " 1   age                  10000 non-null  int64  \n",
      " 2   gender               10000 non-null  int64  \n",
      " 3   driving_experience   10000 non-null  object \n",
      " 4   education            10000 non-null  object \n",
      " 5   income               10000 non-null  object \n",
      " 6   credit_score         9018 non-null   float64\n",
      " 7   vehicle_ownership    10000 non-null  float64\n",
      " 8   vehicle_year         10000 non-null  object \n",
      " 9   married              10000 non-null  float64\n",
      " 10  children             10000 non-null  float64\n",
      " 11  postal_code          10000 non-null  int64  \n",
      " 12  annual_mileage       9043 non-null   float64\n",
      " 13  vehicle_type         10000 non-null  object \n",
      " 14  speeding_violations  10000 non-null  int64  \n",
      " 15  duis                 10000 non-null  int64  \n",
      " 16  past_accidents       10000 non-null  int64  \n",
      " 17  outcome              10000 non-null  float64\n",
      "dtypes: float64(6), int64(7), object(5)\n",
      "memory usage: 1.4+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Overall information of the dataset\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f21c55",
   "metadata": {},
   "source": [
    "## Cleaning the dataset\n",
    "\n",
    "Most of the independent variables are categorical. However, they are presented as either integers or strings in the dataset. Both settings can be used to be categories. The problem is that neither integers nor strings serves us as category. To ensure these columns are treated as categories, we have to verify two conditions: the number of categories match its description in *the dataset* section and its `dtype` is category. In the first condition, I'm assuming that any error or mistype is bound to generate more categories than the ones already placed in its description. The second condition **warrants** the column will be treated as a category.\n",
    "\n",
    "As for the numerical ones, `credit_score` and `annual_mileage` presented null values. Usually, when the dataset have at least one of its row with more than 5% with null values, we'll impute the values with statistical measures (mean or median). In both cases, more than 5% of the column is missing, so I'll impute either the mean or the median, depending on how far the mean is from the median."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1cc685",
   "metadata": {},
   "source": [
    "### Cleaning the categories\n",
    "\n",
    "I will use the `assert_col` function to ensure that the column has the number of the categories of its description and `assert_cat` to verify if its dtype is a category or not (we know it's not, so all will be converted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ec50330-1729-406f-93a4-22b5be5e05bd",
   "metadata": {
    "collapsed": false,
    "executionCancelledAt": null,
    "executionTime": 31,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastExecutedAt": 1733432762678,
    "lastExecutedByKernel": "464275e9-0e65-4af8-8f08-a524cdaeee97",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "cat_4 = [0,1,2,3]\ncat_2 = [0,1]\n\ncat_4_col = list(df[['age', 'driving_experience','income']].columns)\n\ncat_2_col = list(df[['gender','vehicle_ownership', 'vehicle_year', 'married', 'vehicle_type']].columns)\n\ndef assert_col(col, cat = 4):\n    global df, cat_4, cat_2\n    if cat == 4:\n        try:\n            assert df[col].all() in cat_4\n            print(f'the {col} column is clean.')\n        except AssertionError as e:\n            print(f'the {col} column is not clean.')\n            print(e)\n    else:\n        try:\n            assert df[col].all() in cat_2\n            print(f'the {col} column is clean.')\n        except AssertionError as e:\n            print(f'the {col} column is not clean.')\n            print(e)\n\ndef assert_cat(col):\n    global df\n    col_type = str(df[col].dtype)\n    if col_type != 'category':\n        df[col] = df[col].astype('category')\n        print(f'the {col} column is not category. astype method applied.')\n        return df[col]\n    else:\n        print(f'the {col} column is a category. Returning the col')\n        return df[col]\n    \nfor col in cat_4_col:\n    assert_col(col, cat = 4)\n    df[col] = assert_cat(col)\nfor col in cat_2_col:\n    assert_col(col, cat = 2)\n    df[col] = assert_cat(col)\n\ntry:\n    assert df['education'].unique().all() in [0,1,2]\n    print('The education column is clean and contains only valid values')\nexcept AssertionError:\n    print('The education column contains invalid or unexpected values. Please, verify.')",
    "outputsMetadata": {
     "0": {
      "height": 374,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the gender column match its description.\n",
      "the gender column is not category. astype method applied.\n",
      "the vehicle_ownership column match its description.\n",
      "the vehicle_ownership column is not category. astype method applied.\n",
      "the vehicle_year column match its description.\n",
      "the vehicle_year column is not category. astype method applied.\n",
      "the married column match its description.\n",
      "the married column is not category. astype method applied.\n",
      "the vehicle_type column match its description.\n",
      "the vehicle_type column is not category. astype method applied.\n",
      "the age column match its description.\n",
      "the age column is not category. astype method applied.\n",
      "the driving_experience column match its description.\n",
      "the driving_experience column is not category. astype method applied.\n",
      "the income column match its description.\n",
      "the income column is not category. astype method applied.\n",
      "the education column match its description.\n",
      "the education column is not category. astype method applied.\n"
     ]
    }
   ],
   "source": [
    "# Separate the cat columns in three lists. Based on the dataset section.\n",
    "cat_4_col = list(df[['age', 'driving_experience','income']].columns)\n",
    "\n",
    "cat_2_col = list(df[['gender','vehicle_ownership', 'vehicle_year', 'married', 'vehicle_type']].columns)\n",
    "\n",
    "for col in cat_2_col+cat_4_col+['education']:\n",
    "    if col in cat_2_col:\n",
    "        assert_col(df, col, cat = 2)\n",
    "        df[col] = assert_cat(df, col)\n",
    "    elif col in cat_4_col:\n",
    "        assert_col(df, col, cat = 4)\n",
    "        df[col] = assert_cat(df, col)\n",
    "    else:\n",
    "        assert_col(df, col, cat = 3)\n",
    "        df[col] = assert_cat(df, col)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275c670c",
   "metadata": {},
   "source": [
    "### Cleaning the numerical columns\n",
    "\n",
    "We'll take the mean and median to check if they are similar for each column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b165aad4-5f76-4572-ba68-e35a203e2ddd",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 15,
    "lastExecutedAt": 1733432765677,
    "lastExecutedByKernel": "464275e9-0e65-4af8-8f08-a524cdaeee97",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# credit_score and annual mileage has NAs - more than 9% of the data frame\n# substitute NAs for the mean or median\n\ndf[['credit_score', 'annual_mileage']].agg(['mean', 'median'])\n\nprint(f'As the mean and median of credit_score and annual mileage are both similar, I\\'ll use mean as the substitute for NAs')\n",
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        credit_score  annual_mileage\n",
      "mean        0.515813    11697.003207\n",
      "median      0.525033    12000.000000\n"
     ]
    }
   ],
   "source": [
    "# credit_score and annual mileage has NAs - more than 9% of the data frame\n",
    "# substitute NAs for the mean or median\n",
    "\n",
    "print(df[['credit_score', 'annual_mileage']].agg(['mean', 'median']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4162c478",
   "metadata": {},
   "source": [
    "As the mean and median of ``credit_score`` and ``annual_mileage`` columns are both similar, I'll use mean as the substitute for NAs in both cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "530a0915-4589-47c7-81e6-3ffa65528e9b",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 12,
    "lastExecutedAt": 1733432766630,
    "lastExecutedByKernel": "464275e9-0e65-4af8-8f08-a524cdaeee97",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Replacing NAs in credit_score and annual_mileage\nmean_credit = df['credit_score'].mean()\nmean_mileage = df['annual_mileage'].mean()\n\ndf = df.fillna({'credit_score': mean_credit,\n                'annual_mileage': mean_mileage})"
   },
   "outputs": [],
   "source": [
    "# Replacing NAs in credit_score and annual_mileage\n",
    "mean_credit = df['credit_score'].mean()\n",
    "mean_mileage = df['annual_mileage'].mean()\n",
    "\n",
    "df = df.fillna({'credit_score': mean_credit,\n",
    "                'annual_mileage': mean_mileage})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a390f33d",
   "metadata": {},
   "source": [
    "### ``postal_code`` column case\n",
    "Postal codes are a category but it is not displayed as such in the dataset table. In fact, there are only four postal codes repeated throughout the dataset. So we have to convert them into category as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a026186f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    10238\n",
      "1    10238\n",
      "2    10238\n",
      "3    32765\n",
      "4    32765\n",
      "Name: postal_code, dtype: int64\n",
      "[10238 32765 92101 21217]\n"
     ]
    }
   ],
   "source": [
    "# Checking the last column to be verified\n",
    "print(df['postal_code'].head())\n",
    "print(df['postal_code'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aae422f-0f48-449f-b335-e2a0d1c439cd",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": 13,
    "lastExecutedAt": 1733432768527,
    "lastExecutedByKernel": "464275e9-0e65-4af8-8f08-a524cdaeee97",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# Postal code are repetitive, so they are a category\ndf['postal_code'] = df['postal_code'].astype('category')",
    "outputsMetadata": {
     "0": {
      "height": 222,
      "type": "dataFrame"
     }
    }
   },
   "outputs": [],
   "source": [
    "# Postal code are repetitive and are strings, so they are a category\n",
    "df['postal_code'] = df['postal_code'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f90ed5",
   "metadata": {},
   "source": [
    "### The final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d4ea90c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10000 entries, 0 to 9999\n",
      "Data columns (total 18 columns):\n",
      " #   Column               Non-Null Count  Dtype   \n",
      "---  ------               --------------  -----   \n",
      " 0   id                   10000 non-null  int64   \n",
      " 1   age                  10000 non-null  category\n",
      " 2   gender               10000 non-null  category\n",
      " 3   driving_experience   10000 non-null  category\n",
      " 4   education            10000 non-null  category\n",
      " 5   income               10000 non-null  category\n",
      " 6   credit_score         10000 non-null  float64 \n",
      " 7   vehicle_ownership    10000 non-null  category\n",
      " 8   vehicle_year         10000 non-null  category\n",
      " 9   married              10000 non-null  category\n",
      " 10  children             10000 non-null  float64 \n",
      " 11  postal_code          10000 non-null  category\n",
      " 12  annual_mileage       10000 non-null  float64 \n",
      " 13  vehicle_type         10000 non-null  category\n",
      " 14  speeding_violations  10000 non-null  int64   \n",
      " 15  duis                 10000 non-null  int64   \n",
      " 16  past_accidents       10000 non-null  int64   \n",
      " 17  outcome              10000 non-null  float64 \n",
      "dtypes: category(10), float64(4), int64(4)\n",
      "memory usage: 724.3 KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41ceb07",
   "metadata": {},
   "source": [
    "## Applying the logit models\n",
    "\n",
    "Now, we have a fully tidy data frame that can be modelled. Mind that the question to be answer here is: **which variable is the best predictor of whether a costumer will put in a claim on their car insurance?**. As this will be presented as a *single feature* in the machine learning model, we have to analyze one by one its effect on the outcome. In other words, we have to run 16 logit models and calculate its accuracy. To avoid repetition, I'll use list comprehension and the `accuracy` function I created to measure the accuracy of each feature efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d4bb87f-7403-451f-8c21-1ae8fe8222c7",
   "metadata": {
    "collapsed": true,
    "executionCancelledAt": null,
    "executionTime": 2240,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastExecutedAt": 1733432771687,
    "lastExecutedByKernel": "464275e9-0e65-4af8-8f08-a524cdaeee97",
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": "# scaning all the features to search the best based on accuracy\ndef accuracy(logit_model):\n    conf_matrix = logit_model.pred_table()\n    acc = (conf_matrix[0,0] + conf_matrix[1,1])/(conf_matrix[0,0] + conf_matrix[1,1] + conf_matrix[1,0] + conf_matrix[0,1])\n    return acc\n\n# listing all the features\ncols_df = list(df.columns)\ncols_df.remove('outcome')\ncols_df.remove('id')\n\nlogits = [logit(f'outcome ~ {col}', data = df).fit() for col in cols_df]\n\naccs = [accuracy(mdl) for mdl in logits]\n\nfeatures = pd.DataFrame({'feature' : cols_df,\n                         'accuracy' : accs}).sort_values('accuracy', ascending=False)\n\nprint(features) # we have a winner.",
    "outputsMetadata": {
     "0": {
      "height": 616,
      "type": "stream"
     },
     "1": {
      "height": 101,
      "type": "stream"
     },
     "2": {
      "height": 616,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.506484\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.615951\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.467092\n",
      "         Iterations 8\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.603742\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.531499\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.572557\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.552412\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.572668\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.586659\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.595431\n",
      "         Iterations 5\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.601589\n",
      "         Iterations: 35\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.605716\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.621700\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.558922\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.598699\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.549220\n",
      "         Iterations 7\n",
      "                feature  accuracy\n",
      "2    driving_experience    0.7771\n",
      "0                   age    0.7747\n",
      "4                income    0.7425\n",
      "6     vehicle_ownership    0.7351\n",
      "5          credit_score    0.7054\n",
      "10          postal_code    0.6987\n",
      "11       annual_mileage    0.6904\n",
      "1                gender    0.6867\n",
      "3             education    0.6867\n",
      "7          vehicle_year    0.6867\n",
      "8               married    0.6867\n",
      "9              children    0.6867\n",
      "12         vehicle_type    0.6867\n",
      "13  speeding_violations    0.6867\n",
      "14                 duis    0.6867\n",
      "15       past_accidents    0.6867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Paulo Vitor\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python312\\site-packages\\statsmodels\\base\\model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    }
   ],
   "source": [
    "# Scanning all the features to search the best based on accuracy\n",
    "\n",
    "# listing all the features\n",
    "cols_df = list(df.columns)\n",
    "cols_df.remove('outcome')\n",
    "cols_df.remove('id')\n",
    "\n",
    "# list with all the logit models\n",
    "logits = [logit(f'outcome ~ {col}', data = df).fit() for col in cols_df]\n",
    "\n",
    "# list with the logit models' accuracies\n",
    "accs = [accuracy(mdl) for mdl in logits]\n",
    "\n",
    "# Sorting by accuracy to se the best feature by the highest accuracy\n",
    "features = pd.DataFrame({'feature' : cols_df,\n",
    "                         'accuracy' : accs})\\\n",
    "    .sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(features) # we have a winner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e257effb",
   "metadata": {},
   "source": [
    "The following step was just for the project that required a separated data frame with the best feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05875b38-d4dd-438a-8a31-3bbe7bcb9206",
   "metadata": {
    "executionCancelledAt": null,
    "executionTime": null,
    "lastExecutedAt": null,
    "lastExecutedByKernel": null,
    "lastScheduledRunId": null,
    "lastSuccessfullyExecutedCode": null,
    "outputsMetadata": {
     "0": {
      "height": 38,
      "type": "stream"
     }
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best feature is driving_experience with a accuracy of 77.7%\n"
     ]
    }
   ],
   "source": [
    "# Best feature\n",
    "\n",
    "best_feature_df = pd.DataFrame({'best_feature': [features.iloc[0, 0]],\n",
    "                             'best_accuracy': [features.iloc[0, 1]]})\n",
    "\n",
    "best_feature = best_feature_df['best_feature'][0]\n",
    "best_accuracy = round(best_feature_df['best_accuracy'][0]*100, 1)\n",
    "\n",
    "print(f'the best feature is {best_feature} with a accuracy of {best_accuracy}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4c7fa4",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This project was meant to select the most accurate single feature in order to make a simple machine learning model to predict if the customer will or will not put a claim on the insurance. To do so, I cleaned the data and then regress the logit of ``outcome`` over each independent variable individually. Based on the accuracy measure alone, the driver's experience proved to be the best feature for a ML model, with an accuracy of 78%."
   ]
  }
 ],
 "metadata": {
  "editor": "DataLab",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
